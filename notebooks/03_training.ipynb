{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from importlib import reload\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,  accuracy_score, f1_score, precision_score, recall_score, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'dags', 'src'))\n",
    "\n",
    "import config\n",
    "\n",
    "reload(config)\n",
    "\n",
    "engineered_vars = {\n",
    "    \"categorical\": [\"application_year\", \"application_month\", \"application_week\", \"application_day\", \"application_season\"],\n",
    "    \"numerical\": [\"current_credit_balance_ratio\"],\n",
    "    \"date\": [\"application_date\"]\n",
    "}\n",
    "\n",
    "\n",
    "job_id = \"04782775f4d4426f8b5256546c1e2960\"\n",
    "\n",
    "\n",
    "filename = f\"../dags/data/collected/{job_id}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpers.py methods\n",
    "import pandas as pd\n",
    "\n",
    "def check_dataset_sanity(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Checks the sanity of a dataset by identifying null values in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to be checked for null values.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the dataset is considered sane (no null values), False otherwise.\n",
    "    \n",
    "    Raises:\n",
    "    Exception: If there are null values in the DataFrame, an exception is raised with the column names containing null values.\n",
    "    \"\"\"\n",
    "    # Check for null values in the DataFrame\n",
    "    nulls = df.isnull().sum()\n",
    "    \n",
    "    # Get the column names with null values\n",
    "    null_columns = nulls[nulls > 0].index.tolist()\n",
    "    \n",
    "    # If there are no null values, return True\n",
    "    if len(null_columns) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        # If there are null values, raise an exception with the column names\n",
    "        raise Exception(f\"There are null values in the training dataset: {null_columns}\")\n",
    "\n",
    "\n",
    "def persist_deploy_report(job_id: str, model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Persist the deploy report of a job.\n",
    "\n",
    "    Parameters:\n",
    "    job_id (str): The ID of the job.\n",
    "    model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a dictionary representing the deploy report\n",
    "    report = {\n",
    "        \"job_id\": job_id,\n",
    "        \"purpose_to_int\": f\"{job_id}_purpose_to_int_model.json\",\n",
    "        \"missing_values\": f\"{job_id}_missing_values_model.pkl\",\n",
    "        \"prediction_model\": f\"{model_name}.pkl\",\n",
    "        \"train_report\": f\"{job_id}_train_report.json\",\n",
    "    }\n",
    "    \n",
    "    # Save the deploy report as a JSON file\n",
    "    json.dump(report, open(os.path.join(config.PATH_DIR_MODELS, \"deploy_report.json\"), \"w\"))\n",
    "    \n",
    "    # Print the path where the deployment report is saved\n",
    "    print(f'[INFO] Deployment report saved as {os.path.join(config.PATH_DIR_MODELS, \"deploy_report.json\")}')\n",
    "\n",
    "\n",
    "    \n",
    "def save_dataset(df: pd.DataFrame, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a dataset.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to be saved.\n",
    "        path: The path where the dataset should be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"[INFO] Dataset saved to {path}\")\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a dataset.\n",
    "\n",
    "    Args:\n",
    "        path: The path of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def save_model_as_pickle(model, model_name, directory=None) -> None:\n",
    "    \"\"\"\n",
    "    Save a model as a pickle file.\n",
    "\n",
    "    Args:\n",
    "        model: The model object to be saved.\n",
    "        model_name: The name of the model.\n",
    "        directory: The directory where the pickle file should be saved. \n",
    "                   If not provided, the default models directory will be used.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if directory:\n",
    "        filename = os.path.join(directory, model_name + \".pkl\")\n",
    "    else:\n",
    "        filename = os.path.join(config.PATH_DIR_MODELS, model_name + \".pkl\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    print(f\"[INFO] Model saved as pickle file: {filename}\")\n",
    "    \n",
    "    \n",
    "def load_model_from_pickle(model_name: str):\n",
    "    \"\"\"\n",
    "    Load a pickle model.\n",
    "\n",
    "    Args:\n",
    "        model_name: The name of the model to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        The loaded model object.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(config.PATH_DIR_MODELS, model_name + \".pkl\"), \"rb\") as f:\n",
    "        print(f\"[INFO] Model loaded: {model_name}\")\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def save_model_as_json(model:dict, model_name:str, directory:str=None):\n",
    "    \"\"\"\n",
    "    Save a model as a json file.\n",
    "\n",
    "    Args:\n",
    "        model: The model object to be saved.\n",
    "        model_name: The name of the model.\n",
    "        directory: The directory where the pickle file should be saved. \n",
    "                   If not provided, the default models directory will be used.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if directory:\n",
    "        filename = os.path.join(directory, model_name+\".json\")\n",
    "    else:\n",
    "        filename = os.path.join(config.PATH_DIR_MODELS, model_name+\".json\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(model, f)\n",
    "        \n",
    "    print(\"[INFO] Model saved as json file:\", filename)\n",
    "\n",
    "def load_model_from_json(model_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load a json model.\n",
    "\n",
    "    Args:\n",
    "        model_name: The name of the model to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dict.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(config.PATH_DIR_MODELS, model_name+\".json\"), \"r\") as f:\n",
    "        print(f\"[INFO] Model loaded: {model_name}\")\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py methods \n",
    "\n",
    "def performance_report(y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Generate performance report for a model.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (np.array): An array containing the true values.\n",
    "    y_pred (np.array): An array containing the predicted values.\n",
    "    y_prob (np.array): An array containing the prediction probabilities.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing various performance metrics.\n",
    "    \"\"\"\n",
    "    # Create an empty dictionary to store the performance report\n",
    "    report = dict()\n",
    "\n",
    "    # Calculate and store the dataset size\n",
    "    report[\"dataset size\"] = y_true.shape[0]\n",
    "\n",
    "    # Calculate and store the positive rate\n",
    "    report[\"positive rate\"] = y_true.sum() / y_true.shape[0]\n",
    "\n",
    "    # Calculate and store the accuracy\n",
    "    report[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate and store the F1 score\n",
    "    report[\"f1\"] = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate and store the precision\n",
    "    report[\"precision\"] = precision_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate and store the recall\n",
    "    report[\"recall\"] = recall_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate and store the AUC score\n",
    "    report[\"auc\"] = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    # Return the performance report\n",
    "    return report\n",
    "\n",
    "\n",
    "def select_model(df: pd.DataFrame, metric: str = config.MODEL_PERFORMANCE_METRIC, model_names: list = [\"rf\", \"gb\"], performance_thresh: float = config.MODEL_PERFORMANCE_THRESHOLD, degradation_thresh: float = config.MODEL_DEGRADATION_THRESHOLD) -> str:\n",
    "    \"\"\"\n",
    "    Select the best model based on their performance reports.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The performance report DataFrame.\n",
    "    metric (str): The metric to select the best model (default: config.MODEL_PERFORMANCE_METRIC).\n",
    "    model_names (list): The list of model names to select from (default: [\"rf\", \"gb\"]).\n",
    "    performance_thresh (float): The threshold for the performance (default: config.MODEL_PERFORMANCE_THRESHOLD).\n",
    "    degradation_thresh (float): The threshold for degradation (default: config.MODEL_DEGRADATION_THRESHOLD).\n",
    "\n",
    "    Returns:\n",
    "    str: The name of the selected model.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If no model is selected due to all models having performance below the threshold.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store model degradation performance\n",
    "    degradation_performance = []\n",
    "\n",
    "    # Iterate over each model\n",
    "    for model in model_names:\n",
    "        # Check if the model's performance is below the performance threshold\n",
    "        if df.loc[metric, f\"{model}_train\"] < performance_thresh:\n",
    "            continue\n",
    "\n",
    "        # Calculate the degradation\n",
    "        degradation = df.loc[metric, f\"{model}_train\"] - df.loc[metric, f\"{model}_test\"]\n",
    "\n",
    "        # Check if the degradation is below the degradation threshold\n",
    "        if degradation < degradation_thresh:\n",
    "            degradation_performance.append((model, degradation))\n",
    "\n",
    "    # Check if any model meets the selection criteria\n",
    "    if len(degradation_performance) == 0:\n",
    "        raise Exception(\"No model selected: all models have performance below the threshold. Possible overfitting.\")\n",
    "\n",
    "    # Return the model with the minimum degradation\n",
    "    return min(degradation_performance, key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "def train(train_dataset_filename:str=None, test_dataset_filename:str=None, job_id=\"\", rescale=False):\n",
    "    \"\"\"\n",
    "    Train a model on the train dataset loaded from `train_dataset_filename` and test dataset loaded from `test_dataset_filename`.\n",
    "\n",
    "    Parameters:\n",
    "    train_dataset_filename (str): The filename of the train dataset (default: None).\n",
    "    test_dataset_filename (str): The filename of the test dataset (default: None).\n",
    "    job_id (str): The job ID (default: \"\").\n",
    "    rescale (bool): If True, scaled numerical variables are used (default: False).\n",
    "\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    if train_dataset_filename==None:\n",
    "        train_dataset_filename = os.path.join(config.PATH_DIR_DATA, \"preprocessed\", f\"{job_id}_training.csv\")\n",
    "    if test_dataset_filename==None:\n",
    "        test_dataset_filename = os.path.join(config.PATH_DIR_DATA, \"preprocessed\", f\"{job_id}_inference.csv\")\n",
    "    tdf = load_dataset(train_dataset_filename)\n",
    "    vdf = load_dataset(test_dataset_filename)\n",
    "    check_dataset_sanity(tdf)\n",
    "    check_dataset_sanity(vdf)\n",
    "    \n",
    "    predictors = config.PREDICTORS\n",
    "    target = config.TARGET\n",
    "    if rescale:\n",
    "        for col in predictors:\n",
    "            if f\"{config.RESCALE_METHOD}_{col}\" in tdf.columns:\n",
    "                tdf[col] = tdf[f\"{config.RESCALE_METHOD}_{col}\"]\n",
    "            if f\"{config.RESCALE_METHOD}_{col}\" in vdf.columns:\n",
    "                vdf[col] = vdf[f\"{config.RESCALE_METHOD}_{col}\"]\n",
    "        \n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=config.RANDOM_SEED)\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, max_depth=10, random_state=config.RANDOM_SEED)\n",
    "    X, Y = tdf[predictors], tdf[target]\n",
    "    report = dict()\n",
    "    models = dict()\n",
    "    for cl, name in [(rf, \"rf\"), (gb, \"gb\")]:\n",
    "        print(\"[INFO] Training model:\", name)\n",
    "        cl.fit(X, Y)\n",
    "        t_pred = cl.predict(X)\n",
    "        v_pred = cl.predict(vdf[predictors])\n",
    "        t_prob = cl.predict_proba(X)[:, 1]\n",
    "        v_prob = cl.predict_proba(vdf[predictors])[:, 1]\n",
    "        report[f\"{name}_train\"] = performance_report(Y, t_pred, t_prob)\n",
    "        report[f\"{name}_test\"] = performance_report(vdf[target], v_pred, v_prob)\n",
    "        models[name] = cl\n",
    "        \n",
    "    model_name = select_model(pd.DataFrame(report), metric=config.MODEL_PERFORMANCE_METRIC, model_names=list(models.keys()))\n",
    "    report[\"final_model\"] = model_name\n",
    "    save_model_as_pickle(models[model_name], f\"{job_id}_{model_name}\")\n",
    "    save_model_as_json(report, f\"{job_id}_train_report\")\n",
    "    return report\n",
    "\n",
    "\n",
    "def pick_model_and_deploy(job_id, models, df, metric=\"auc\", predictors=config.PREDICTORS, target=config.TARGET) -> str:\n",
    "    \"\"\"\n",
    "    Among all `models`, select the model that performs best on `df` and mark it for deployment.\n",
    "\n",
    "    Parameters:\n",
    "    job_id (str): The ID of the job.\n",
    "    models (list): A list of dictionaries representing the models.\n",
    "    df (pd.DataFrame): The DataFrame on which the models are evaluated.\n",
    "    metric (str): The metric used to evaluate the models (default: \"auc\").\n",
    "    predictors (list): The list of predictor variables (default: config.PREDICTORS).\n",
    "    target (str): The target variable (default: config.TARGET).\n",
    "\n",
    "    Returns:\n",
    "    str: The name of the selected model for deployment.\n",
    "    \"\"\"\n",
    "    # Check if the columns in `predictors` are present in `df`\n",
    "    cols = set(predictors).difference(set(df.columns))\n",
    "    assert cols == set(), f\"{cols} not in {df.columns}\"\n",
    "\n",
    "    # Initialize variables for tracking the best model\n",
    "    score = 0\n",
    "    m_idx = 0\n",
    "\n",
    "    # Iterate over the models and evaluate their performance on `df`\n",
    "    for i, model in enumerate(models):\n",
    "        y_true = df[target]\n",
    "        y_pred = model[\"model\"].predict(df[predictors])\n",
    "        y_prob = model[\"model\"].predict_proba(df[predictors])[:, 1]\n",
    "        r = performance_report(y_true, y_pred, y_prob)\n",
    "        if r[metric] > score:\n",
    "            score = r[metric]\n",
    "            m_idx = i\n",
    "\n",
    "    # Persist the deploy report for the selected model\n",
    "    persist_deploy_report(job_id, models[m_idx][\"model_name\"])\n",
    "\n",
    "    # Return the name of the selected model for deployment\n",
    "    return models[m_idx][\"model_name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model: rf\n",
      "[INFO] Training model: gb\n",
      "[INFO] Model saved as pickle file: ../dags/models\\04782775f4d4426f8b5256546c1e2960_rf.pkl\n",
      "[INFO] Model saved as json file: ../dags/models\\04782775f4d4426f8b5256546c1e2960_train_report.json\n",
      "{'rf_train': {'dataset size': 15264, 'positive rate': 0.7576650943396226, 'accuracy': 0.8986504192872118, 'f1': 0.9362140766090793, 'precision': 0.894782471626734, 'recall': 0.9816688283614353, 'auc': 0.9669825347451965}, 'rf_test': {'dataset size': 6688, 'positive rate': 0.7764653110047847, 'accuracy': 0.9020633971291866, 'f1': 0.9394471664971803, 'precision': 0.9034495021337127, 'recall': 0.9784325052955902, 'auc': 0.9377489764649738}, 'gb_train': {'dataset size': 15264, 'positive rate': 0.7576650943396226, 'accuracy': 0.9962657232704403, 'f1': 0.9975417259671367, 'precision': 0.9950955085183273, 'recall': 1.0, 'auc': 0.9999966104813035}, 'gb_test': {'dataset size': 6688, 'positive rate': 0.7764653110047847, 'accuracy': 0.8920454545454546, 'f1': 0.9325233644859813, 'precision': 0.9059378972217178, 'recall': 0.9607163489312536, 'auc': 0.932639319588306}, 'final_model': 'rf'}\n"
     ]
    }
   ],
   "source": [
    "report = train(job_id=job_id)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deployment report saved as ../dags/models\\deploy_report.json\n",
      "Deployed model: 04782775f4d4426f8b5256546c1e2960_rf\n"
     ]
    }
   ],
   "source": [
    "model = pick_model_and_deploy(\n",
    "    job_id=job_id,\n",
    "    df = pd.read_csv(f\"../dags/data/preprocessed/{job_id}_inference.csv\"),\n",
    "    models = [{\n",
    "        \"model_name\": f\"{job_id}_{report['final_model']}\", \n",
    "        \"model\": pickle.load(open(f\"../dags/models/{job_id}_rf.pkl\", \"rb\"))\n",
    "    }]\n",
    ")\n",
    "print(\"Deployed model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
